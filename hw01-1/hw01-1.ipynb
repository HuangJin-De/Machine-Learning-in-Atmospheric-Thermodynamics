{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmqDiyjTwIQvwWPMZOrv+r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics/blob/main/hw01-1/hw01-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lOfIjeVg8jf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# test whether torch is available\n",
        "# if not, you could apply GPU by turn on it in Edit->Notebook Settings in the control bar\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# download data from Fashion-Mnist website\n",
        "train_data=datasets.FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
        "test_data=datasets.FashionMNIST(root=\"data\",train=False,download=True,transform=ToTensor())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()  \n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),   \n",
        "            nn.ReLU(),           \n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# dataloader\n",
        "batch_size=64\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size)\n",
        "test_loader=DataLoader(test_data,batch_size=batch_size)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)       \n",
        "        loss = loss_fn(pred, y) \n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()      \n",
        "        optimizer.step()      \n",
        "       \n",
        "        #if batch % 100 == 0:\n",
        "        #    loss, current = loss.item(), batch * len(X)\n",
        "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    return train_loss\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "# training start\n",
        "epochs=50\n",
        "\n",
        "tt=time.time()\n",
        "train_loss=[]\n",
        "test_loss=[]\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}-------------------------------\")\n",
        "    loss = train(train_loader, model, loss_fn, optimizer)\n",
        "    train_loss.append(loss)\n",
        "    loss = test(test_loader, model, loss_fn)\n",
        "    test_loss.append(loss)\n",
        "\n",
        "elapse=time.time()-tt\n",
        "print(elapse)\n",
        "\n"
      ],
      "metadata": {
        "id": "s_0gnHlpgmgW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}