{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3SHkpDhzaoqHFFwQC1rsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics/blob/master/hw03/hw03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf Machine-Learning-in-Atmospheric-Thermodynamics\n",
        "! git clone https://github.com/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics.git\n",
        "\n"
      ],
      "metadata": {
        "id": "x9hcwec3GK_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ae7585-aefc-4c9e-8eb6-88becbc3a175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning-in-Atmospheric-Thermodynamics'...\n",
            "remote: Enumerating objects: 343, done.\u001b[K\n",
            "remote: Counting objects: 100% (311/311), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 343 (delta 107), reused 206 (delta 47), pack-reused 32\u001b[K\n",
            "Receiving objects: 100% (343/343), 180.07 MiB | 17.05 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path='Machine-Learning-in-Atmospheric-Thermodynamics/hw03'\n",
        "filen=path+'/data/era5_stn_th.dat'\n",
        "\n",
        "data=np.fromfile(filen,dtype=np.float32)\n",
        "data=data.reshape(-1,17)\n",
        "\n",
        "x=data[:,0:16]\n",
        "y=data[:,16].reshape(-1,1)\n",
        "\n",
        "print(x.shape,x[1,:])\n",
        "print(y.shape,y[1,:])\n",
        "\n",
        "x_temp,x_valid,y_temp,y_valid=train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_temp,y_temp,test_size=0.2,random_state=1)\n",
        "\n",
        "scaler=StandardScaler()\n",
        "x_train=scaler.fit_transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_valid=scaler.transform(x_valid)\n",
        "\n",
        "print('finished')"
      ],
      "metadata": {
        "id": "YxnXTb_RKVzE",
        "outputId": "76b5adb4-33ca-40ca-bde7-5361ed31d0bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9417, 16) [326.37604 323.53775 320.28677 317.47955 315.22504 312.1862  310.70364\n",
            " 309.3074  307.98483 306.47357 305.08533 303.91504 302.85    301.75815\n",
            " 300.15994 299.9878 ]\n",
            "(9417, 1) [0.]\n",
            "finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "train_data=TrainData(torch.FloatTensor(x_train),torch.FloatTensor(y_train))\n",
        "test_data=TrainData(torch.FloatTensor(x_test),torch.FloatTensor(y_test))\n",
        "\n",
        "class ValidData(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data):\n",
        "        self.x_data = x_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x_data)\n",
        "    \n",
        "valid_data=ValidData(torch.FloatTensor(x_valid))\n",
        "\n",
        "BATCH_SIZE=64\n",
        "train_loader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_loader=DataLoader(dataset=valid_data,batch_size=1)\n",
        "\n",
        "print('finished')"
      ],
      "metadata": {
        "id": "QmQW_5rbNt6y",
        "outputId": "9c758272-f012-4ec4-f12b-7a1a38fa40f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import time\n",
        "\n",
        "class BinaryClassification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BinaryClassification, self).__init__()\n",
        "    self.BC = nn.Sequential(\n",
        "      nn.Linear(16, 128),   \n",
        "      nn.ReLU(),     \n",
        "      nn.BatchNorm1d(128),   \n",
        "      nn.Linear(128, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm1d(128),\n",
        "      nn.Dropout(p=0.1),\n",
        "      nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    freq = self.BC(x)\n",
        "    binary = freq #torch.round(freq)\n",
        "    return binary\n",
        "\n",
        "print('defined model')"
      ],
      "metadata": {
        "id": "KXm6STvyOvnL",
        "outputId": "65480c22-8d80-45c2-f1ca-2b669faf01d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = BinaryClassification().to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)  \n",
        "        loss = loss_fn(pred, y) \n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()      \n",
        "        optimizer.step()      \n",
        "       \n",
        "        #if batch % 100 == 0:\n",
        "        #    loss, current = loss.item(), batch * len(X)\n",
        "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    return train_loss\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (torch.round(torch.sigmoid(pred))==y).sum().float()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return test_loss\n",
        "\n",
        "# training start\n",
        "epochs=200\n",
        "\n",
        "tt=time.time()\n",
        "train_loss=[]\n",
        "test_loss=[]\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}-------------------------------\")\n",
        "    loss = train(train_loader, model, loss_fn, optimizer)\n",
        "    train_loss.append(loss)\n",
        "    loss = test(test_loader, model, loss_fn)\n",
        "    test_loss.append(loss)\n",
        "\n",
        "elapse=time.time()-tt\n",
        "print(elapse)"
      ],
      "metadata": {
        "id": "LV1kX5wbR1j7",
        "outputId": "1a109164-2afe-4532-d0d2-12948c4b03a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1-------------------------------\n",
            "Accuracy: 51.8%, Avg loss: 0.694560\n",
            "Epoch 2-------------------------------\n",
            "Accuracy: 61.2%, Avg loss: 0.644897\n",
            "Epoch 3-------------------------------\n",
            "Accuracy: 64.5%, Avg loss: 0.625451\n",
            "Epoch 4-------------------------------\n",
            "Accuracy: 67.3%, Avg loss: 0.603757\n",
            "Epoch 5-------------------------------\n",
            "Accuracy: 68.1%, Avg loss: 0.605434\n",
            "Epoch 6-------------------------------\n",
            "Accuracy: 68.7%, Avg loss: 0.590997\n",
            "Epoch 7-------------------------------\n",
            "Accuracy: 70.0%, Avg loss: 0.580488\n",
            "Epoch 8-------------------------------\n",
            "Accuracy: 69.7%, Avg loss: 0.579207\n",
            "Epoch 9-------------------------------\n",
            "Accuracy: 71.3%, Avg loss: 0.564572\n",
            "Epoch 10-------------------------------\n",
            "Accuracy: 71.7%, Avg loss: 0.568700\n",
            "Epoch 11-------------------------------\n",
            "Accuracy: 70.7%, Avg loss: 0.575378\n",
            "Epoch 12-------------------------------\n",
            "Accuracy: 71.7%, Avg loss: 0.561495\n",
            "Epoch 13-------------------------------\n",
            "Accuracy: 72.0%, Avg loss: 0.568593\n",
            "Epoch 14-------------------------------\n",
            "Accuracy: 72.5%, Avg loss: 0.561938\n",
            "Epoch 15-------------------------------\n",
            "Accuracy: 72.4%, Avg loss: 0.564147\n",
            "Epoch 16-------------------------------\n",
            "Accuracy: 72.9%, Avg loss: 0.546409\n",
            "Epoch 17-------------------------------\n",
            "Accuracy: 72.7%, Avg loss: 0.568671\n",
            "Epoch 18-------------------------------\n",
            "Accuracy: 73.3%, Avg loss: 0.548312\n",
            "Epoch 19-------------------------------\n",
            "Accuracy: 72.8%, Avg loss: 0.562194\n",
            "Epoch 20-------------------------------\n",
            "Accuracy: 73.9%, Avg loss: 0.539331\n",
            "Epoch 21-------------------------------\n",
            "Accuracy: 73.5%, Avg loss: 0.557415\n",
            "Epoch 22-------------------------------\n",
            "Accuracy: 73.3%, Avg loss: 0.553662\n",
            "Epoch 23-------------------------------\n",
            "Accuracy: 73.7%, Avg loss: 0.548054\n",
            "Epoch 24-------------------------------\n",
            "Accuracy: 73.9%, Avg loss: 0.541713\n",
            "Epoch 25-------------------------------\n",
            "Accuracy: 74.1%, Avg loss: 0.542936\n",
            "Epoch 26-------------------------------\n",
            "Accuracy: 74.7%, Avg loss: 0.541971\n",
            "Epoch 27-------------------------------\n",
            "Accuracy: 74.0%, Avg loss: 0.537192\n",
            "Epoch 28-------------------------------\n",
            "Accuracy: 74.0%, Avg loss: 0.548924\n",
            "Epoch 29-------------------------------\n",
            "Accuracy: 74.2%, Avg loss: 0.544626\n",
            "Epoch 30-------------------------------\n",
            "Accuracy: 74.9%, Avg loss: 0.536074\n",
            "Epoch 31-------------------------------\n",
            "Accuracy: 75.0%, Avg loss: 0.538265\n",
            "Epoch 32-------------------------------\n",
            "Accuracy: 74.9%, Avg loss: 0.538873\n",
            "Epoch 33-------------------------------\n",
            "Accuracy: 75.2%, Avg loss: 0.529878\n",
            "Epoch 34-------------------------------\n",
            "Accuracy: 75.1%, Avg loss: 0.529144\n",
            "Epoch 35-------------------------------\n",
            "Accuracy: 75.0%, Avg loss: 0.527871\n",
            "Epoch 36-------------------------------\n",
            "Accuracy: 75.0%, Avg loss: 0.540854\n",
            "Epoch 37-------------------------------\n",
            "Accuracy: 75.3%, Avg loss: 0.536680\n",
            "Epoch 38-------------------------------\n",
            "Accuracy: 75.2%, Avg loss: 0.535768\n",
            "Epoch 39-------------------------------\n",
            "Accuracy: 75.2%, Avg loss: 0.527985\n",
            "Epoch 40-------------------------------\n",
            "Accuracy: 75.6%, Avg loss: 0.518631\n",
            "Epoch 41-------------------------------\n",
            "Accuracy: 75.4%, Avg loss: 0.528897\n",
            "Epoch 42-------------------------------\n",
            "Accuracy: 75.7%, Avg loss: 0.512434\n",
            "Epoch 43-------------------------------\n",
            "Accuracy: 75.5%, Avg loss: 0.526791\n",
            "Epoch 44-------------------------------\n",
            "Accuracy: 75.4%, Avg loss: 0.530484\n",
            "Epoch 45-------------------------------\n",
            "Accuracy: 76.2%, Avg loss: 0.515884\n",
            "Epoch 46-------------------------------\n",
            "Accuracy: 75.4%, Avg loss: 0.530845\n",
            "Epoch 47-------------------------------\n",
            "Accuracy: 75.5%, Avg loss: 0.521792\n",
            "Epoch 48-------------------------------\n",
            "Accuracy: 76.2%, Avg loss: 0.513627\n",
            "Epoch 49-------------------------------\n",
            "Accuracy: 75.6%, Avg loss: 0.519707\n",
            "Epoch 50-------------------------------\n",
            "Accuracy: 75.8%, Avg loss: 0.508155\n",
            "Epoch 51-------------------------------\n",
            "Accuracy: 76.1%, Avg loss: 0.509953\n",
            "Epoch 52-------------------------------\n",
            "Accuracy: 76.3%, Avg loss: 0.500919\n",
            "Epoch 53-------------------------------\n",
            "Accuracy: 75.8%, Avg loss: 0.501990\n",
            "Epoch 54-------------------------------\n",
            "Accuracy: 75.8%, Avg loss: 0.512319\n",
            "Epoch 55-------------------------------\n",
            "Accuracy: 76.2%, Avg loss: 0.498661\n",
            "Epoch 56-------------------------------\n",
            "Accuracy: 76.5%, Avg loss: 0.510734\n",
            "Epoch 57-------------------------------\n",
            "Accuracy: 76.4%, Avg loss: 0.501115\n",
            "Epoch 58-------------------------------\n",
            "Accuracy: 76.0%, Avg loss: 0.506943\n",
            "Epoch 59-------------------------------\n",
            "Accuracy: 76.0%, Avg loss: 0.502490\n",
            "Epoch 60-------------------------------\n",
            "Accuracy: 76.3%, Avg loss: 0.503201\n",
            "Epoch 61-------------------------------\n",
            "Accuracy: 76.0%, Avg loss: 0.516778\n",
            "Epoch 62-------------------------------\n",
            "Accuracy: 76.4%, Avg loss: 0.500492\n",
            "Epoch 63-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.498109\n",
            "Epoch 64-------------------------------\n",
            "Accuracy: 76.2%, Avg loss: 0.500425\n",
            "Epoch 65-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.495093\n",
            "Epoch 66-------------------------------\n",
            "Accuracy: 75.8%, Avg loss: 0.512335\n",
            "Epoch 67-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.489374\n",
            "Epoch 68-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.484918\n",
            "Epoch 69-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.501633\n",
            "Epoch 70-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.488513\n",
            "Epoch 71-------------------------------\n",
            "Accuracy: 76.9%, Avg loss: 0.493130\n",
            "Epoch 72-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.493561\n",
            "Epoch 73-------------------------------\n",
            "Accuracy: 76.9%, Avg loss: 0.490586\n",
            "Epoch 74-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.482006\n",
            "Epoch 75-------------------------------\n",
            "Accuracy: 77.3%, Avg loss: 0.485784\n",
            "Epoch 76-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.483654\n",
            "Epoch 77-------------------------------\n",
            "Accuracy: 76.7%, Avg loss: 0.477065\n",
            "Epoch 78-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.484801\n",
            "Epoch 79-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.489773\n",
            "Epoch 80-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.490316\n",
            "Epoch 81-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.487520\n",
            "Epoch 82-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.490020\n",
            "Epoch 83-------------------------------\n",
            "Accuracy: 76.9%, Avg loss: 0.477342\n",
            "Epoch 84-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.481923\n",
            "Epoch 85-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.474361\n",
            "Epoch 86-------------------------------\n",
            "Accuracy: 77.3%, Avg loss: 0.486053\n",
            "Epoch 87-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.481157\n",
            "Epoch 88-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.482007\n",
            "Epoch 89-------------------------------\n",
            "Accuracy: 77.2%, Avg loss: 0.476282\n",
            "Epoch 90-------------------------------\n",
            "Accuracy: 77.0%, Avg loss: 0.476361\n",
            "Epoch 91-------------------------------\n",
            "Accuracy: 77.5%, Avg loss: 0.472366\n",
            "Epoch 92-------------------------------\n",
            "Accuracy: 77.3%, Avg loss: 0.473368\n",
            "Epoch 93-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.465419\n",
            "Epoch 94-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.476496\n",
            "Epoch 95-------------------------------\n",
            "Accuracy: 76.9%, Avg loss: 0.478322\n",
            "Epoch 96-------------------------------\n",
            "Accuracy: 77.5%, Avg loss: 0.475290\n",
            "Epoch 97-------------------------------\n",
            "Accuracy: 78.4%, Avg loss: 0.461454\n",
            "Epoch 98-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.468327\n",
            "Epoch 99-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.467435\n",
            "Epoch 100-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.467240\n",
            "Epoch 101-------------------------------\n",
            "Accuracy: 77.5%, Avg loss: 0.469923\n",
            "Epoch 102-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.467189\n",
            "Epoch 103-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.460528\n",
            "Epoch 104-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.464686\n",
            "Epoch 105-------------------------------\n",
            "Accuracy: 77.4%, Avg loss: 0.471567\n",
            "Epoch 106-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.459434\n",
            "Epoch 107-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.463353\n",
            "Epoch 108-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.471962\n",
            "Epoch 109-------------------------------\n",
            "Accuracy: 77.4%, Avg loss: 0.471777\n",
            "Epoch 110-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.461880\n",
            "Epoch 111-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.462179\n",
            "Epoch 112-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.460423\n",
            "Epoch 113-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.467070\n",
            "Epoch 114-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.454230\n",
            "Epoch 115-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.456669\n",
            "Epoch 116-------------------------------\n",
            "Accuracy: 77.5%, Avg loss: 0.457233\n",
            "Epoch 117-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.467803\n",
            "Epoch 118-------------------------------\n",
            "Accuracy: 77.5%, Avg loss: 0.462598\n",
            "Epoch 119-------------------------------\n",
            "Accuracy: 78.3%, Avg loss: 0.449879\n",
            "Epoch 120-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.458256\n",
            "Epoch 121-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.450751\n",
            "Epoch 122-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.457463\n",
            "Epoch 123-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.454283\n",
            "Epoch 124-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.458805\n",
            "Epoch 125-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.458024\n",
            "Epoch 126-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.451280\n",
            "Epoch 127-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.451445\n",
            "Epoch 128-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.453987\n",
            "Epoch 129-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.453275\n",
            "Epoch 130-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.451094\n",
            "Epoch 131-------------------------------\n",
            "Accuracy: 78.3%, Avg loss: 0.452644\n",
            "Epoch 132-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.451066\n",
            "Epoch 133-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.446404\n",
            "Epoch 134-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.443632\n",
            "Epoch 135-------------------------------\n",
            "Accuracy: 78.3%, Avg loss: 0.448227\n",
            "Epoch 136-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.451654\n",
            "Epoch 137-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.451914\n",
            "Epoch 138-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.450270\n",
            "Epoch 139-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.448122\n",
            "Epoch 140-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.451705\n",
            "Epoch 141-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.444969\n",
            "Epoch 142-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.448155\n",
            "Epoch 143-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.448759\n",
            "Epoch 144-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.445931\n",
            "Epoch 145-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.445111\n",
            "Epoch 146-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.447451\n",
            "Epoch 147-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.441924\n",
            "Epoch 148-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.444425\n",
            "Epoch 149-------------------------------\n",
            "Accuracy: 78.4%, Avg loss: 0.444246\n",
            "Epoch 150-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.440692\n",
            "Epoch 151-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.444877\n",
            "Epoch 152-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.437410\n",
            "Epoch 153-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.442857\n",
            "Epoch 154-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.445108\n",
            "Epoch 155-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.442310\n",
            "Epoch 156-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.441551\n",
            "Epoch 157-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.441024\n",
            "Epoch 158-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.439914\n",
            "Epoch 159-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.440883\n",
            "Epoch 160-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.440738\n",
            "Epoch 161-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.440375\n",
            "Epoch 162-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.439864\n",
            "Epoch 163-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.441103\n",
            "Epoch 164-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.444953\n",
            "Epoch 165-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.438814\n",
            "Epoch 166-------------------------------\n",
            "Accuracy: 78.1%, Avg loss: 0.441436\n",
            "Epoch 167-------------------------------\n",
            "Accuracy: 78.3%, Avg loss: 0.439495\n",
            "Epoch 168-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437754\n",
            "Epoch 169-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.437762\n",
            "Epoch 170-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.440277\n",
            "Epoch 171-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.440763\n",
            "Epoch 172-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.439445\n",
            "Epoch 173-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.438070\n",
            "Epoch 174-------------------------------\n",
            "Accuracy: 77.9%, Avg loss: 0.436995\n",
            "Epoch 175-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.441172\n",
            "Epoch 176-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.438679\n",
            "Epoch 177-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.436866\n",
            "Epoch 178-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.438846\n",
            "Epoch 179-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.440022\n",
            "Epoch 180-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437151\n",
            "Epoch 181-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.440043\n",
            "Epoch 182-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.441255\n",
            "Epoch 183-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437822\n",
            "Epoch 184-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.439573\n",
            "Epoch 185-------------------------------\n",
            "Accuracy: 77.6%, Avg loss: 0.438561\n",
            "Epoch 186-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437661\n",
            "Epoch 187-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437479\n",
            "Epoch 188-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.445228\n",
            "Epoch 189-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.438435\n",
            "Epoch 190-------------------------------\n",
            "Accuracy: 77.8%, Avg loss: 0.439249\n",
            "Epoch 191-------------------------------\n",
            "Accuracy: 78.4%, Avg loss: 0.437240\n",
            "Epoch 192-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.439842\n",
            "Epoch 193-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437973\n",
            "Epoch 194-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.437108\n",
            "Epoch 195-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.439320\n",
            "Epoch 196-------------------------------\n",
            "Accuracy: 78.2%, Avg loss: 0.439965\n",
            "Epoch 197-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.436369\n",
            "Epoch 198-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.438824\n",
            "Epoch 199-------------------------------\n",
            "Accuracy: 77.7%, Avg loss: 0.438167\n",
            "Epoch 200-------------------------------\n",
            "Accuracy: 78.0%, Avg loss: 0.438515\n",
            "54.39594554901123\n"
          ]
        }
      ]
    }
  ]
}