{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrd8fkqg4cE1xjDBy91zmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics/blob/master/hw03/hw03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf Machine-Learning-in-Atmospheric-Thermodynamics\n",
        "! git clone https://github.com/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics.git\n",
        "\n"
      ],
      "metadata": {
        "id": "x9hcwec3GK_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1bd6de-f502-4a03-ba49-8cd8441f5a98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning-in-Atmospheric-Thermodynamics'...\n",
            "remote: Enumerating objects: 318, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 318 (delta 99), reused 189 (delta 43), pack-reused 32\u001b[K\n",
            "Receiving objects: 100% (318/318), 178.70 MiB | 16.73 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path='Machine-Learning-in-Atmospheric-Thermodynamics/hw03'\n",
        "df=pd.read_csv(path+'/inversion_LTS.47918.csv')\n",
        "df.head()\n",
        "\n",
        "x=df.iloc[:, 1].astype(np.float32)\n",
        "y=df.iloc[:, -1].astype(np.float32)\n",
        "\n",
        "#print(x[0:5])\n",
        "#print(y[0:5])\n",
        "\n",
        "x_temp,x_valid,y_temp,y_valid=train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_temp,y_temp,test_size=0.2,random_state=1)\n",
        "\n",
        "scaler=StandardScaler()\n",
        "x_train=scaler.fit_transform(x_train.values.reshape(-1,1))\n",
        "x_test=scaler.transform(x_test.values.reshape(-1,1))\n",
        "x_valid=scaler.transform(x_valid.values.reshape(-1,1))\n",
        "y_train=y_train.values.reshape(-1,1)\n",
        "y_test=y_test.values.reshape(-1,1)\n",
        "y_valid=y_valid.values.reshape(-1,1)\n",
        "\n",
        "print('finished')"
      ],
      "metadata": {
        "id": "YxnXTb_RKVzE",
        "outputId": "6237fb04-21aa-4d79-8643-6b379c30d757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "train_data=TrainData(torch.FloatTensor(x_train),torch.FloatTensor(y_train))\n",
        "test_data=TrainData(torch.FloatTensor(x_test),torch.FloatTensor(y_test))\n",
        "\n",
        "class ValidData(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data):\n",
        "        self.x_data = x_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x_data)\n",
        "    \n",
        "valid_data=ValidData(torch.FloatTensor(x_valid))\n",
        "\n",
        "BATCH_SIZE=64\n",
        "train_loader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_loader=DataLoader(dataset=valid_data,batch_size=1)\n",
        "\n",
        "print('finished')"
      ],
      "metadata": {
        "id": "QmQW_5rbNt6y",
        "outputId": "d11e3d39-2275-4dfc-e9f0-dbd49702e31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import time\n",
        "\n",
        "class BinaryClassification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BinaryClassification, self).__init__()\n",
        "    self.BC = nn.Sequential(\n",
        "      nn.Linear(1, 128),   \n",
        "      nn.ReLU(),     \n",
        "      nn.BatchNorm1d(128),   \n",
        "      nn.Linear(128, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm1d(128),\n",
        "      nn.Dropout(p=0.1),\n",
        "      nn.Linear(128, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    freq = self.BC(x)\n",
        "    binary = torch.round(freq)\n",
        "    return binary\n",
        "\n",
        "print('defined model')"
      ],
      "metadata": {
        "id": "KXm6STvyOvnL",
        "outputId": "ccb7c761-1671-4250-8f7c-80cb53245871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = BinaryClassification().to(device)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)  \n",
        "        loss = loss_fn(pred, y) \n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()      \n",
        "        optimizer.step()      \n",
        "       \n",
        "        #if batch % 100 == 0:\n",
        "        #    loss, current = loss.item(), batch * len(X)\n",
        "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    return train_loss\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred==y).sum().float()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return test_loss\n",
        "\n",
        "# training start\n",
        "epochs=1000\n",
        "\n",
        "tt=time.time()\n",
        "train_loss=[]\n",
        "test_loss=[]\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}-------------------------------\")\n",
        "    loss = train(train_loader, model, loss_fn, optimizer)\n",
        "    train_loss.append(loss)\n",
        "    loss = test(test_loader, model, loss_fn)\n",
        "    test_loss.append(loss)\n",
        "\n",
        "elapse=time.time()-tt\n",
        "print(elapse)"
      ],
      "metadata": {
        "id": "LV1kX5wbR1j7",
        "outputId": "df82b19b-f71e-458c-f41a-31babb49957b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1-------------------------------\n",
            "Accuracy: 60.2%, Avg loss: 40.087426\n",
            "Epoch 2-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.393601\n",
            "Epoch 3-------------------------------\n",
            "Accuracy: 59.6%, Avg loss: 40.511533\n",
            "Epoch 4-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 37.832961\n",
            "Epoch 5-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.396577\n",
            "Epoch 6-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.798363\n",
            "Epoch 7-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.465774\n",
            "Epoch 8-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 40.913318\n",
            "Epoch 9-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.504464\n",
            "Epoch 10-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 41.977307\n",
            "Epoch 11-------------------------------\n",
            "Accuracy: 57.7%, Avg loss: 42.183780\n",
            "Epoch 12-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 41.815476\n",
            "Epoch 13-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 37.832961\n",
            "Epoch 14-------------------------------\n",
            "Accuracy: 60.4%, Avg loss: 39.838170\n",
            "Epoch 15-------------------------------\n",
            "Accuracy: 61.2%, Avg loss: 38.560268\n",
            "Epoch 16-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 39.862351\n",
            "Epoch 17-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.208333\n",
            "Epoch 18-------------------------------\n",
            "Accuracy: 60.9%, Avg loss: 39.371280\n",
            "Epoch 19-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.472842\n",
            "Epoch 20-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.450521\n",
            "Epoch 21-------------------------------\n",
            "Accuracy: 57.0%, Avg loss: 43.374256\n",
            "Epoch 22-------------------------------\n",
            "Accuracy: 57.6%, Avg loss: 42.410714\n",
            "Epoch 23-------------------------------\n",
            "Accuracy: 65.2%, Avg loss: 34.880952\n",
            "Epoch 24-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.563244\n",
            "Epoch 25-------------------------------\n",
            "Accuracy: 60.3%, Avg loss: 39.687500\n",
            "Epoch 26-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.139137\n",
            "Epoch 27-------------------------------\n",
            "Accuracy: 60.0%, Avg loss: 40.066964\n",
            "Epoch 28-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 41.086310\n",
            "Epoch 29-------------------------------\n",
            "Accuracy: 58.0%, Avg loss: 42.289807\n",
            "Epoch 30-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.367932\n",
            "Epoch 31-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.424107\n",
            "Epoch 32-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.004836\n",
            "Epoch 33-------------------------------\n",
            "Accuracy: 60.5%, Avg loss: 39.330357\n",
            "Epoch 34-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.342634\n",
            "Epoch 35-------------------------------\n",
            "Accuracy: 64.0%, Avg loss: 35.879836\n",
            "Epoch 36-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 41.988467\n",
            "Epoch 37-------------------------------\n",
            "Accuracy: 57.4%, Avg loss: 42.659970\n",
            "Epoch 38-------------------------------\n",
            "Accuracy: 57.8%, Avg loss: 42.323289\n",
            "Epoch 39-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.208333\n",
            "Epoch 40-------------------------------\n",
            "Accuracy: 59.5%, Avg loss: 40.468750\n",
            "Epoch 41-------------------------------\n",
            "Accuracy: 61.8%, Avg loss: 38.136161\n",
            "Epoch 42-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.315104\n",
            "Epoch 43-------------------------------\n",
            "Accuracy: 59.2%, Avg loss: 40.902158\n",
            "Epoch 44-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.671131\n",
            "Epoch 45-------------------------------\n",
            "Accuracy: 58.5%, Avg loss: 41.348586\n",
            "Epoch 46-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 40.197173\n",
            "Epoch 47-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.742560\n",
            "Epoch 48-------------------------------\n",
            "Accuracy: 60.4%, Avg loss: 39.946057\n",
            "Epoch 49-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.100446\n",
            "Epoch 50-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.153274\n",
            "Epoch 51-------------------------------\n",
            "Accuracy: 61.3%, Avg loss: 38.926711\n",
            "Epoch 52-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.744420\n",
            "Epoch 53-------------------------------\n",
            "Accuracy: 57.7%, Avg loss: 42.453497\n",
            "Epoch 54-------------------------------\n",
            "Accuracy: 61.7%, Avg loss: 38.266369\n",
            "Epoch 55-------------------------------\n",
            "Accuracy: 56.7%, Avg loss: 43.430060\n",
            "Epoch 56-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 40.143229\n",
            "Epoch 57-------------------------------\n",
            "Accuracy: 59.4%, Avg loss: 40.437128\n",
            "Epoch 58-------------------------------\n",
            "Accuracy: 56.7%, Avg loss: 43.268229\n",
            "Epoch 59-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.486979\n",
            "Epoch 60-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.196057\n",
            "Epoch 61-------------------------------\n",
            "Accuracy: 60.3%, Avg loss: 39.590774\n",
            "Epoch 62-------------------------------\n",
            "Accuracy: 57.7%, Avg loss: 42.496280\n",
            "Epoch 63-------------------------------\n",
            "Accuracy: 56.9%, Avg loss: 43.288691\n",
            "Epoch 64-------------------------------\n",
            "Accuracy: 64.3%, Avg loss: 35.889137\n",
            "Epoch 65-------------------------------\n",
            "Accuracy: 59.2%, Avg loss: 41.117932\n",
            "Epoch 66-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 41.075149\n",
            "Epoch 67-------------------------------\n",
            "Accuracy: 57.4%, Avg loss: 42.498140\n",
            "Epoch 68-------------------------------\n",
            "Accuracy: 63.9%, Avg loss: 36.333705\n",
            "Epoch 69-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 39.947917\n",
            "Epoch 70-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 38.059896\n",
            "Epoch 71-------------------------------\n",
            "Accuracy: 60.9%, Avg loss: 38.993676\n",
            "Epoch 72-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.744420\n",
            "Epoch 73-------------------------------\n",
            "Accuracy: 61.9%, Avg loss: 37.693452\n",
            "Epoch 74-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.196057\n",
            "Epoch 75-------------------------------\n",
            "Accuracy: 59.4%, Avg loss: 40.437128\n",
            "Epoch 76-------------------------------\n",
            "Accuracy: 59.2%, Avg loss: 40.848214\n",
            "Epoch 77-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.798363\n",
            "Epoch 78-------------------------------\n",
            "Accuracy: 57.8%, Avg loss: 42.215402\n",
            "Epoch 79-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 41.988467\n",
            "Epoch 80-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.139137\n",
            "Epoch 81-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.096354\n",
            "Epoch 82-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 40.109747\n",
            "Epoch 83-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.328497\n",
            "Epoch 84-------------------------------\n",
            "Accuracy: 62.5%, Avg loss: 37.485119\n",
            "Epoch 85-------------------------------\n",
            "Accuracy: 61.9%, Avg loss: 38.125000\n",
            "Epoch 86-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 41.086310\n",
            "Epoch 87-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 39.970238\n",
            "Epoch 88-------------------------------\n",
            "Accuracy: 58.6%, Avg loss: 41.434152\n",
            "Epoch 89-------------------------------\n",
            "Accuracy: 60.3%, Avg loss: 39.860491\n",
            "Epoch 90-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 37.617188\n",
            "Epoch 91-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.501488\n",
            "Epoch 92-------------------------------\n",
            "Accuracy: 59.3%, Avg loss: 40.837054\n",
            "Epoch 93-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.906250\n",
            "Epoch 94-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.745536\n",
            "Epoch 95-------------------------------\n",
            "Accuracy: 60.0%, Avg loss: 40.174851\n",
            "Epoch 96-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.367932\n",
            "Epoch 97-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.316220\n",
            "Epoch 98-------------------------------\n",
            "Accuracy: 59.4%, Avg loss: 40.652902\n",
            "Epoch 99-------------------------------\n",
            "Accuracy: 63.3%, Avg loss: 36.703869\n",
            "Epoch 100-------------------------------\n",
            "Accuracy: 61.2%, Avg loss: 38.722098\n",
            "Epoch 101-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.418899\n",
            "Epoch 102-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 40.035342\n",
            "Epoch 103-------------------------------\n",
            "Accuracy: 60.7%, Avg loss: 39.296875\n",
            "Epoch 104-------------------------------\n",
            "Accuracy: 57.7%, Avg loss: 42.399554\n",
            "Epoch 105-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.725074\n",
            "Epoch 106-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.193080\n",
            "Epoch 107-------------------------------\n",
            "Accuracy: 61.9%, Avg loss: 37.963170\n",
            "Epoch 108-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 40.154390\n",
            "Epoch 109-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.288690\n",
            "Epoch 110-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 40.805432\n",
            "Epoch 111-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.193080\n",
            "Epoch 112-------------------------------\n",
            "Accuracy: 56.4%, Avg loss: 43.258929\n",
            "Epoch 113-------------------------------\n",
            "Accuracy: 56.9%, Avg loss: 43.234747\n",
            "Epoch 114-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.682292\n",
            "Epoch 115-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.725074\n",
            "Epoch 116-------------------------------\n",
            "Accuracy: 60.5%, Avg loss: 39.276414\n",
            "Epoch 117-------------------------------\n",
            "Accuracy: 59.2%, Avg loss: 40.578497\n",
            "Epoch 118-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.688616\n",
            "Epoch 119-------------------------------\n",
            "Accuracy: 58.5%, Avg loss: 41.564360\n",
            "Epoch 120-------------------------------\n",
            "Accuracy: 58.8%, Avg loss: 41.184896\n",
            "Epoch 121-------------------------------\n",
            "Accuracy: 58.5%, Avg loss: 41.607143\n",
            "Epoch 122-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 41.934524\n",
            "Epoch 123-------------------------------\n",
            "Accuracy: 61.8%, Avg loss: 38.244048\n",
            "Epoch 124-------------------------------\n",
            "Accuracy: 59.7%, Avg loss: 40.327381\n",
            "Epoch 125-------------------------------\n",
            "Accuracy: 58.0%, Avg loss: 41.966146\n",
            "Epoch 126-------------------------------\n",
            "Accuracy: 60.6%, Avg loss: 39.265253\n",
            "Epoch 127-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 40.163691\n",
            "Epoch 128-------------------------------\n",
            "Accuracy: 61.7%, Avg loss: 38.536086\n",
            "Epoch 129-------------------------------\n",
            "Accuracy: 59.5%, Avg loss: 40.522693\n",
            "Epoch 130-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 40.978423\n",
            "Epoch 131-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.367932\n",
            "Epoch 132-------------------------------\n",
            "Accuracy: 59.6%, Avg loss: 40.511533\n",
            "Epoch 133-------------------------------\n",
            "Accuracy: 61.1%, Avg loss: 38.906250\n",
            "Epoch 134-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.455357\n",
            "Epoch 135-------------------------------\n",
            "Accuracy: 56.8%, Avg loss: 43.407738\n",
            "Epoch 136-------------------------------\n",
            "Accuracy: 58.8%, Avg loss: 41.292783\n",
            "Epoch 137-------------------------------\n",
            "Accuracy: 58.2%, Avg loss: 41.662946\n",
            "Epoch 138-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 40.271577\n",
            "Epoch 139-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 39.624256\n",
            "Epoch 140-------------------------------\n",
            "Accuracy: 62.1%, Avg loss: 37.983631\n",
            "Epoch 141-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.486979\n",
            "Epoch 142-------------------------------\n",
            "Accuracy: 58.3%, Avg loss: 41.813616\n",
            "Epoch 143-------------------------------\n",
            "Accuracy: 59.1%, Avg loss: 40.924479\n",
            "Epoch 144-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 37.628348\n",
            "Epoch 145-------------------------------\n",
            "Accuracy: 62.1%, Avg loss: 37.767857\n",
            "Epoch 146-------------------------------\n",
            "Accuracy: 58.5%, Avg loss: 41.726191\n",
            "Epoch 147-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 39.927455\n",
            "Epoch 148-------------------------------\n",
            "Accuracy: 60.5%, Avg loss: 39.492188\n",
            "Epoch 149-------------------------------\n",
            "Accuracy: 62.7%, Avg loss: 37.289807\n",
            "Epoch 150-------------------------------\n",
            "Accuracy: 64.0%, Avg loss: 35.933780\n",
            "Epoch 151-------------------------------\n",
            "Accuracy: 58.1%, Avg loss: 42.051711\n",
            "Epoch 152-------------------------------\n",
            "Accuracy: 59.7%, Avg loss: 40.327381\n",
            "Epoch 153-------------------------------\n",
            "Accuracy: 64.8%, Avg loss: 34.882812\n",
            "Epoch 154-------------------------------\n",
            "Accuracy: 62.3%, Avg loss: 37.680432\n",
            "Epoch 155-------------------------------\n",
            "Accuracy: 60.0%, Avg loss: 39.959077\n",
            "Epoch 156-------------------------------\n",
            "Accuracy: 59.7%, Avg loss: 40.176711\n",
            "Epoch 157-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.540923\n",
            "Epoch 158-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 39.774926\n",
            "Epoch 159-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 39.882812\n",
            "Epoch 160-------------------------------\n",
            "Accuracy: 61.2%, Avg loss: 38.625372\n",
            "Epoch 161-------------------------------\n",
            "Accuracy: 58.3%, Avg loss: 41.802455\n",
            "Epoch 162-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.645833\n",
            "Epoch 163-------------------------------\n",
            "Accuracy: 60.0%, Avg loss: 40.013021\n",
            "Epoch 164-------------------------------\n",
            "Accuracy: 62.0%, Avg loss: 37.898065\n",
            "Epoch 165-------------------------------\n",
            "Accuracy: 59.0%, Avg loss: 40.827753\n",
            "Epoch 166-------------------------------\n",
            "Accuracy: 57.7%, Avg loss: 42.399554\n",
            "Epoch 167-------------------------------\n",
            "Accuracy: 60.5%, Avg loss: 39.654018\n",
            "Epoch 168-------------------------------\n",
            "Accuracy: 59.9%, Avg loss: 39.862351\n",
            "Epoch 169-------------------------------\n",
            "Accuracy: 60.7%, Avg loss: 39.404762\n",
            "Epoch 170-------------------------------\n",
            "Accuracy: 57.1%, Avg loss: 43.082217\n",
            "Epoch 171-------------------------------\n",
            "Accuracy: 58.9%, Avg loss: 40.946801\n",
            "Epoch 172-------------------------------\n",
            "Accuracy: 59.7%, Avg loss: 40.381324\n",
            "Epoch 173-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.385417\n",
            "Epoch 174-------------------------------\n",
            "Accuracy: 60.4%, Avg loss: 39.460565\n",
            "Epoch 175-------------------------------\n",
            "Accuracy: 60.2%, Avg loss: 39.871652\n",
            "Epoch 176-------------------------------\n",
            "Accuracy: 59.7%, Avg loss: 40.219494\n",
            "Epoch 177-------------------------------\n",
            "Accuracy: 58.2%, Avg loss: 41.770833\n",
            "Epoch 178-------------------------------\n",
            "Accuracy: 61.0%, Avg loss: 39.025298\n",
            "Epoch 179-------------------------------\n",
            "Accuracy: 61.3%, Avg loss: 38.495164\n",
            "Epoch 180-------------------------------\n",
            "Accuracy: 58.3%, Avg loss: 41.759673\n",
            "Epoch 181-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.123884\n",
            "Epoch 182-------------------------------\n",
            "Accuracy: 60.9%, Avg loss: 38.993676\n",
            "Epoch 183-------------------------------\n",
            "Accuracy: 59.8%, Avg loss: 39.992560\n",
            "Epoch 184-------------------------------\n",
            "Accuracy: 60.5%, Avg loss: 39.546131\n",
            "Epoch 185-------------------------------\n",
            "Accuracy: 58.7%, Avg loss: 41.422991\n",
            "Epoch 186-------------------------------\n",
            "Accuracy: 58.6%, Avg loss: 41.326265\n",
            "Epoch 187-------------------------------\n",
            "Accuracy: 56.9%, Avg loss: 43.126860\n",
            "Epoch 188-------------------------------\n",
            "Accuracy: 61.9%, Avg loss: 38.017113\n",
            "Epoch 189-------------------------------\n",
            "Accuracy: 61.4%, Avg loss: 38.753720\n",
            "Epoch 190-------------------------------\n",
            "Accuracy: 58.0%, Avg loss: 41.858259\n",
            "Epoch 191-------------------------------\n",
            "Accuracy: 60.1%, Avg loss: 39.828869\n",
            "Epoch 192-------------------------------\n",
            "Accuracy: 61.3%, Avg loss: 39.034598\n",
            "Epoch 193-------------------------------\n",
            "Accuracy: 58.2%, Avg loss: 41.609003\n",
            "Epoch 194-------------------------------\n",
            "Accuracy: 56.6%, Avg loss: 43.009673\n",
            "Epoch 195-------------------------------\n",
            "Accuracy: 57.0%, Avg loss: 43.104539\n",
            "Epoch 196-------------------------------\n",
            "Accuracy: 60.3%, Avg loss: 39.590774\n",
            "Epoch 197-------------------------------\n",
            "Accuracy: 58.5%, Avg loss: 41.391369\n",
            "Epoch 198-------------------------------\n",
            "Accuracy: 60.8%, Avg loss: 39.177827\n",
            "Epoch 199-------------------------------\n",
            "Accuracy: 57.2%, Avg loss: 42.801339\n",
            "Epoch 200-------------------------------\n",
            "Accuracy: 61.6%, Avg loss: 38.450521\n",
            "Epoch 201-------------------------------\n",
            "Accuracy: 60.0%, Avg loss: 39.959077\n",
            "Epoch 202-------------------------------\n",
            "Accuracy: 57.5%, Avg loss: 42.486979\n",
            "Epoch 203-------------------------------\n",
            "Accuracy: 62.2%, Avg loss: 37.745536\n",
            "Epoch 204-------------------------------\n",
            "Accuracy: 59.3%, Avg loss: 40.783110\n",
            "Epoch 205-------------------------------\n",
            "Accuracy: 57.3%, Avg loss: 42.628348\n",
            "Epoch 206-------------------------------\n",
            "Accuracy: 57.0%, Avg loss: 42.780878\n",
            "Epoch 207-------------------------------\n",
            "Accuracy: 59.3%, Avg loss: 40.502232\n",
            "Epoch 208-------------------------------\n",
            "Accuracy: 57.9%, Avg loss: 42.204241\n",
            "Epoch 209-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-cb9036c0ee26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-cb9036c0ee26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#if batch % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}