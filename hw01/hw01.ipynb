{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE6jvxHf5Fq+SGoSRHvT41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuangJin-De/Machine-Learning-in-Atmospheric-Thermodynamics/blob/main/hw01/hw01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WicZZg_Da8MG",
        "outputId": "d2c74a93-14e2-4e1f-b21c-f0843f2d86ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# test whether torch is available\n",
        "# if not, you could apply GPU by turn on it in Edit->Notebook Settings in the control bar\n",
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download data from Fashion-Mnist website\n",
        "train_data=datasets.FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
        "test_data=datasets.FashionMNIST(root=\"data\",train=False,download=True,transform=ToTensor())"
      ],
      "metadata": {
        "id": "C_H8FkaWdIs7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "\n",
        "# dataloader\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size)\n",
        "test_loader=DataLoader(test_data,batch_size=batch_size)\n",
        "\n",
        "# test and plot data\n",
        "for X, y in test_loader:\n",
        "  print(X.shape)\n",
        "  print(y.shape)\n",
        "  fig, ax = plt.subplots(figsize=(3,3))\n",
        "  c = plt.pcolor(X[0,0,:,:],cmap='gray')\n",
        "  break"
      ],
      "metadata": {
        "id": "qSUmdrKf0kCu",
        "outputId": "54f30d1c-48f7-4b4a-8769-e21258204785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADCCAYAAADjAebGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMm0lEQVR4nO3dX2hcdRYH8O/X/lPbGmprYnXtHyUW6kpVSiuuDy52V1eEah9k+7AULNQH/4KCxQcVFkFBdF8WpGKwiNtFdHcNS922FmG7sEj/SW2bLi3Sso1pYq2a1D+tac8+zM1umv2dnLkzNzOTyfcDQ2ZOJpnfnfT05p6cey7NDCLiu6jeCxBpdEoSkYCSRCSgJBEJKElEAkoSkcDkWr4YSdWbpVGdNLMrUp/QnkSk5Jj3CSWJSEBJIhJQkogElCQiASWJSEBJIhJQkogElCQiASWJSEBJIhJQkogElCQigTBJSF5D8iOSB0keIPl4Fn+eZDfJT7LbPWO/XJHaK6dVfhDAk2a2h+RMALtJbss+96qZvTx2yxOpvzBJzKwHQE92f4BkF4Crx3phIo0i1zEJyQUAbgbwcRZ6hOQ+kh0kZzlfs47kLpK7qlqpSL2YWVk3ADMA7AawKnvcBmASSon2AoCOMr6H6aZbg952ef9uy9qTkJwC4D0Ab5vZnwDAzHrN7JyZnQfwOoBl5XwvkfGmnOoWAbwBoMvMXhkWnzvsafcD2F/88kTqr5zq1s8A/AbApyQ/yWLPAFhN8iaUdlVHATw0JisUqTPWcmC2pqVIA9ttZktTn9Bf3EUCShKRgJJEJKAkEQkoSUQCShKRgJJEJKAkEQkoSUQCShKRgJJEJKAkEQnU9HJwRZgxY0Yyvnz58mR80aJFyfhtt92WjF9xRfKKYLjsssuS8enTpyfjpTMM/p/XUHr+/Plk/NSpU8n4oUOHkvEPP/wwGd+8eXMy/sMPPyTjeXV2dibj8+bNS8ZPnjyZjA8MDOSKDw4OJuNnzpxJxp966qlk/Ntvv03GAe1JREJKEpGAkkQkoCQRCShJRAI1PX138uTJNnPmzAtijz32WPK5N954YzI+a1ZyvJfLq3541RKvmuTFv/nmm2R86tSpybhX9Zo9e3Yy7lXnLr300mTcq8J5P+clS5Yk4/v27UvGjx49moyvWLEiGZ82bVqu9Xi8n2N3d3cy3t/fn4yvXbs2Gd+7d69O3xWpVDUDsy8nuY3k4exjvv/iRcaJcvYkQwOzFwO4FcDDJBcDWA9gu5m1A9iePRZpOmGSmFmPme3J7g8AGBqYvRLAxuxpGwHcN1aLFKmnXG0pIwZmt2UT5wHgBEqzgVNfsw7Auux+pesUqZuyk4TkDJTmAT9hZv3D/8GbmXmD58xsA4ANANDS0mLLll04MviGG25Ivt7BgweT8Z6enmT87NmzyfhFF6V3lt7zJ02alIx7CT55cvot9KpbXo+W10P1+eefJ+OXXHJJMj5nzpxk3KsydXV1JeNtbcn/8zCyOjlk7969ybj3/nu8Hiqvmue9bz/++GMy7lU1R1PxwGwAvUPzgLOPfblfXWQcqHhgNoBOAGuy+2sAvF/88kTqr5qB2S8CeIfkWgDHADwwNksUqa9yLgf3DwDeEfedxS5HpPHoL+4igZqemdjf34+tW7deEPOqPVdddVUy7vXweGcUetWqc+fO5Xq+V93yzpS8+OKLc30fr/r01VdfJePe++ZVk7znnzhxIhn/8ssvk/He3t5kvL29PRn3eL1wXtWxpaUlGfeqfNdee20y3tramowfOXIkGQe0JxEJKUlEAkoSkYCSRCRQ95FC3ggcry1i/vz5ybjXruKN/PEO+BYuXJiMe20RXvuDd0DsHVhPmTIlGffW6R2Inz59OhnP24bjndzmtYF4I3y89hyP97pe3Gtj8dbjtRGNRnsSkYCSRCSgJBEJKElEAkoSkUDNq1sjqylee4jX/uDFi+K1tyxYsCAZ90b+zJ07Nxn3qkzeaCLv/fFGB3nVNq8651X/vGqbNwD766+/Tsa/++67XOvxqnNee47XFpS33WY02pOIBJQkIgEliUhASSISUJKIBGpe3fKqNSPlHUXjDWD2TnLyep+++OKLXPGdO3cm495JY6tWrUrGvWqVd3KSd/KZ1yvlvZ/eiKBjx44l497IIo9XJfN6qLyRP14Pm7e93nYdOHAgGR+N9iQigXJGCnWQ7CO5f1jseZLdJD/JbveM7TJF6qecPcmbAO5OxF81s5uyW/rSriJNoJyB2X8HkP7FWGQCqOaY5BGS+7Jfx9xrk5BcR3IXyV1VvJZI3ZR1Obhsmvxfzeyn2eM2ACcBGIDfAphrZg+W8X1qd+25gFft8eJeVa6oy+k9+GD67bv++uuTcW+Qdt4q39KlySuguZdZ86pMXhXL69HyRi55vXle9c/rPduzZ08y/tJLLyXjAIq9HJyZ9ZrZOTM7D+B1AMuirxEZrypKkqFp8pn7Aez3nisy3oV/TCS5CcAdAOaQPA7gOQB3kLwJpV+3jgJ4aAzXKFJX5QzMXp0IvzEGaxFpSPqLu0ig7nO3iuJVb7xqj9e75cXzyjuou6OjIxlfvTq1IwduueWWZNy7LJvXK/X9998n43l7ybyeLq+65b0/eatzni1btuR6/mi0JxEJKElEAkoSkYCSRCSgJBEJNE11K28PVd5qmNfT5T0/7+XmvOdv2rQpGffOfLzyyiuTcW8avDcvy5ve71XJvO3yzqDMW0X0nu9NlT98+HCu7z8a7UlEAkoSkYCSRCSgJBEJNM2Be155D/SLalfxDtDzXn/9tddeS8afffbZZNy7PJrXluId6HuX6fMGWnvb5Z0s5Y0I8gaNe9vlHdBXQnsSkYCSRCSgJBEJKElEAkoSkcCErW41Gq+K5bWBeNWbd999Nxm/9957k3GviuUNrm5paUnGvfV7VURvcPXs2bOTce8ksOPHjyfjRdKeRCRQ6cDsy0luI3k4++hOcBQZ7yodmL0ewHYzawewPXss0pQqHZi9EsDG7P5GAPcVvC6RhlHpMUmbmfVk908ASPcqiDSBqqtbZmajDcImuQ7AumpfZ6Iq9/J5Q7zL01133XXJ+PLly5Nx76SlRx99NNd68mptbU3G77rrrmTcu+xbkSrdk/QOzQPOPvZ5TzSzDWa21JvYLdLoKk2STgBrsvtrALxfzHJEGk85JeBNAP4JYBHJ4yTXAngRwC9IHgawInss0pQqHZgNAHcWvBaRhqS/uIsE1Ls1TuU9k3HHjh3JuDfy54MPPqhsYVXq60vXgN56661k/Omnnx7L5QDQnkQkpCQRCShJRAJKEpGAkkQkwLzzp6p6sVF6vGR88AZj5z0z0TNt2rRk3Juv1d7enoxXMDB7t9c6pT2JSEBJIhJQkogElCQiASWJSEDVLZESVbdEKqUkEQkoSUQCShKRgJJEJKAkEQlUdfouyaMABgCcAzCo2VrSjIo4x/3nZnaygO8j0pD065ZIoNokMQBbSe7OZv6KNJ1qf9263cy6SbYC2EbyUHaphv/SwGwZ7wrr3SL5PIDTZvbyKM9R75Y0quJ7t0hOJzlz6D6AXwLYP/pXiYw/1fy61QbgzySHvs8fzOxvhaxKpIFUnCRm9hmAJQWuRaQhqQQsElCSiASUJCIBJYlIQEkiElCSiASUJCIBJYlIQEkiElCSiASUJCIBJYlIQEkiElCSiASUJCIBJYlIQEkiElCSiASUJCIBJYlIoKokIXk3yX+RPEJyfVGLEmkk1czdmgTg9wB+BWAxgNUkFxe1MJFGUc2eZBmAI2b2mZmdBfBHACuLWZZI46gmSa4G8O9hj49nMZGmUsT1SUY1YmD2GUysUahzAEyka7eM5+2d732imiTpBnDNsMc/yWIXMLMNADYAAMldE+lqWNre5lDNr1s7AbSTXEhyKoBfA+gsZlkijaOaWcCDJB8BsAXAJAAdZnagsJWJNIiqjknMbDOAzTm+ZEM1rzcOaXubQGEX8RFpVmpLEQnUJEkmQvsKyQ6SfST3D4tdTnIbycPZx1n1XGNRSF5D8iOSB0keIPl4Fm/K7R3zJJlA7StvArh7RGw9gO1m1g5ge/a4GQwCeNLMFgO4FcDD2c+0Kbe3FnuSCdG+kl11+NSI8EoAG7P7GwHcV9NFjREz6zGzPdn9AQBdKHVbNOX21iJJJnL7SpuZ9WT3T6B0ncmmQnIBgJsBfIwm3V4duNeIlcqITVVKJDkDwHsAnjCz/uGfa6btrUWSlNW+0qR6Sc4FgOxjX53XUxiSU1BKkLfN7E9ZuCm3txZJMpHbVzoBrMnurwHwfh3XUhiWrkv+BoAuM3tl2Keac3tr8cdEkvcA+B3+177ywpi/aI2R3ATgDpQ6YXsBPAfgLwDeATAPwDEAD5jZyIP7cYfk7QB2APgUwPks/AxKxyXNt736i7vI6HTgLhJQkogElCQiASWJSEBJIhJQkogElCQiASWJSOA/gi/qi80I/wAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()  \n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),   \n",
        "            nn.ReLU(),           \n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "aoioBI3P0xrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67921b0-500e-4990-8417-5bfc69660e16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)       \n",
        "        loss = loss_fn(pred, y) \n",
        "\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()      \n",
        "        optimizer.step()      \n",
        "       \n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-tSPKlw7VwM",
        "outputId": "ab4e1f24-5716-4ebf-99c1-912ebbcb2e86"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50\n",
        "\n",
        "tt=time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    test(test_loader, model, loss_fn)\n",
        "\n",
        "elapse=time.time()-tt\n",
        "print(elapse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRWltObT784-",
        "outputId": "df6fcb2e-19f4-49ea-d024-c530ef8f1743"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306618  [    0/60000]\n",
            "loss: 2.288276  [ 6400/60000]\n",
            "loss: 2.268625  [12800/60000]\n",
            "loss: 2.265591  [19200/60000]\n",
            "loss: 2.239361  [25600/60000]\n",
            "loss: 2.222285  [32000/60000]\n",
            "loss: 2.227527  [38400/60000]\n",
            "loss: 2.193876  [44800/60000]\n",
            "loss: 2.193264  [51200/60000]\n",
            "loss: 2.153550  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.4%, Avg loss: 2.148233 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.163383  [    0/60000]\n",
            "loss: 2.146298  [ 6400/60000]\n",
            "loss: 2.085076  [12800/60000]\n",
            "loss: 2.109641  [19200/60000]\n",
            "loss: 2.055277  [25600/60000]\n",
            "loss: 1.993428  [32000/60000]\n",
            "loss: 2.033123  [38400/60000]\n",
            "loss: 1.944319  [44800/60000]\n",
            "loss: 1.960866  [51200/60000]\n",
            "loss: 1.884761  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 1.878229 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.915761  [    0/60000]\n",
            "loss: 1.877334  [ 6400/60000]\n",
            "loss: 1.757448  [12800/60000]\n",
            "loss: 1.812036  [19200/60000]\n",
            "loss: 1.701434  [25600/60000]\n",
            "loss: 1.647924  [32000/60000]\n",
            "loss: 1.689083  [38400/60000]\n",
            "loss: 1.575368  [44800/60000]\n",
            "loss: 1.611079  [51200/60000]\n",
            "loss: 1.510835  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.7%, Avg loss: 1.519704 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.585766  [    0/60000]\n",
            "loss: 1.547225  [ 6400/60000]\n",
            "loss: 1.397224  [12800/60000]\n",
            "loss: 1.480325  [19200/60000]\n",
            "loss: 1.358182  [25600/60000]\n",
            "loss: 1.351865  [32000/60000]\n",
            "loss: 1.379383  [38400/60000]\n",
            "loss: 1.291044  [44800/60000]\n",
            "loss: 1.333397  [51200/60000]\n",
            "loss: 1.240390  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.259099 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.331961  [    0/60000]\n",
            "loss: 1.314720  [ 6400/60000]\n",
            "loss: 1.150462  [12800/60000]\n",
            "loss: 1.263770  [19200/60000]\n",
            "loss: 1.129108  [25600/60000]\n",
            "loss: 1.155513  [32000/60000]\n",
            "loss: 1.185288  [38400/60000]\n",
            "loss: 1.111634  [44800/60000]\n",
            "loss: 1.158077  [51200/60000]\n",
            "loss: 1.080389  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.7%, Avg loss: 1.095095 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.160675  [    0/60000]\n",
            "loss: 1.167706  [ 6400/60000]\n",
            "loss: 0.986080  [12800/60000]\n",
            "loss: 1.127198  [19200/60000]\n",
            "loss: 0.983405  [25600/60000]\n",
            "loss: 1.021567  [32000/60000]\n",
            "loss: 1.063292  [38400/60000]\n",
            "loss: 0.994809  [44800/60000]\n",
            "loss: 1.041409  [51200/60000]\n",
            "loss: 0.979046  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 0.987681 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.039923  [    0/60000]\n",
            "loss: 1.071987  [ 6400/60000]\n",
            "loss: 0.872372  [12800/60000]\n",
            "loss: 1.035523  [19200/60000]\n",
            "loss: 0.889107  [25600/60000]\n",
            "loss: 0.925737  [32000/60000]\n",
            "loss: 0.982569  [38400/60000]\n",
            "loss: 0.917369  [44800/60000]\n",
            "loss: 0.958382  [51200/60000]\n",
            "loss: 0.910220  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.913408 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.949985  [    0/60000]\n",
            "loss: 1.004707  [ 6400/60000]\n",
            "loss: 0.789696  [12800/60000]\n",
            "loss: 0.969959  [19200/60000]\n",
            "loss: 0.825063  [25600/60000]\n",
            "loss: 0.854788  [32000/60000]\n",
            "loss: 0.925795  [38400/60000]\n",
            "loss: 0.864639  [44800/60000]\n",
            "loss: 0.896930  [51200/60000]\n",
            "loss: 0.860290  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.859376 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.880245  [    0/60000]\n",
            "loss: 0.953682  [ 6400/60000]\n",
            "loss: 0.727254  [12800/60000]\n",
            "loss: 0.920533  [19200/60000]\n",
            "loss: 0.779473  [25600/60000]\n",
            "loss: 0.800892  [32000/60000]\n",
            "loss: 0.883095  [38400/60000]\n",
            "loss: 0.827458  [44800/60000]\n",
            "loss: 0.850200  [51200/60000]\n",
            "loss: 0.821847  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.818274 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.824256  [    0/60000]\n",
            "loss: 0.912528  [ 6400/60000]\n",
            "loss: 0.678391  [12800/60000]\n",
            "loss: 0.881905  [19200/60000]\n",
            "loss: 0.745244  [25600/60000]\n",
            "loss: 0.759265  [32000/60000]\n",
            "loss: 0.848852  [38400/60000]\n",
            "loss: 0.800414  [44800/60000]\n",
            "loss: 0.813650  [51200/60000]\n",
            "loss: 0.790735  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.785694 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.778081  [    0/60000]\n",
            "loss: 0.877546  [ 6400/60000]\n",
            "loss: 0.638965  [12800/60000]\n",
            "loss: 0.850749  [19200/60000]\n",
            "loss: 0.718416  [25600/60000]\n",
            "loss: 0.726492  [32000/60000]\n",
            "loss: 0.819959  [38400/60000]\n",
            "loss: 0.779450  [44800/60000]\n",
            "loss: 0.784179  [51200/60000]\n",
            "loss: 0.764704  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.758778 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.738972  [    0/60000]\n",
            "loss: 0.846612  [ 6400/60000]\n",
            "loss: 0.606397  [12800/60000]\n",
            "loss: 0.825090  [19200/60000]\n",
            "loss: 0.696465  [25600/60000]\n",
            "loss: 0.700242  [32000/60000]\n",
            "loss: 0.794459  [38400/60000]\n",
            "loss: 0.762085  [44800/60000]\n",
            "loss: 0.759740  [51200/60000]\n",
            "loss: 0.742070  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.735745 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.705075  [    0/60000]\n",
            "loss: 0.818643  [ 6400/60000]\n",
            "loss: 0.578883  [12800/60000]\n",
            "loss: 0.803373  [19200/60000]\n",
            "loss: 0.677881  [25600/60000]\n",
            "loss: 0.678573  [32000/60000]\n",
            "loss: 0.771325  [38400/60000]\n",
            "loss: 0.747094  [44800/60000]\n",
            "loss: 0.738956  [51200/60000]\n",
            "loss: 0.721673  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.3%, Avg loss: 0.715494 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.675466  [    0/60000]\n",
            "loss: 0.793011  [ 6400/60000]\n",
            "loss: 0.555196  [12800/60000]\n",
            "loss: 0.784805  [19200/60000]\n",
            "loss: 0.661714  [25600/60000]\n",
            "loss: 0.660394  [32000/60000]\n",
            "loss: 0.750029  [38400/60000]\n",
            "loss: 0.733742  [44800/60000]\n",
            "loss: 0.720923  [51200/60000]\n",
            "loss: 0.703161  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.1%, Avg loss: 0.697361 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.649328  [    0/60000]\n",
            "loss: 0.769421  [ 6400/60000]\n",
            "loss: 0.534590  [12800/60000]\n",
            "loss: 0.768465  [19200/60000]\n",
            "loss: 0.647503  [25600/60000]\n",
            "loss: 0.644920  [32000/60000]\n",
            "loss: 0.730335  [38400/60000]\n",
            "loss: 0.721715  [44800/60000]\n",
            "loss: 0.705190  [51200/60000]\n",
            "loss: 0.686151  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.680933 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.626083  [    0/60000]\n",
            "loss: 0.747649  [ 6400/60000]\n",
            "loss: 0.516443  [12800/60000]\n",
            "loss: 0.753874  [19200/60000]\n",
            "loss: 0.634888  [25600/60000]\n",
            "loss: 0.631635  [32000/60000]\n",
            "loss: 0.712056  [38400/60000]\n",
            "loss: 0.710929  [44800/60000]\n",
            "loss: 0.691505  [51200/60000]\n",
            "loss: 0.670513  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.665939 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.605347  [    0/60000]\n",
            "loss: 0.727596  [ 6400/60000]\n",
            "loss: 0.500302  [12800/60000]\n",
            "loss: 0.740678  [19200/60000]\n",
            "loss: 0.623662  [25600/60000]\n",
            "loss: 0.620102  [32000/60000]\n",
            "loss: 0.694988  [38400/60000]\n",
            "loss: 0.701330  [44800/60000]\n",
            "loss: 0.679612  [51200/60000]\n",
            "loss: 0.656059  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.652196 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.586769  [    0/60000]\n",
            "loss: 0.709097  [ 6400/60000]\n",
            "loss: 0.485878  [12800/60000]\n",
            "loss: 0.728546  [19200/60000]\n",
            "loss: 0.613488  [25600/60000]\n",
            "loss: 0.609922  [32000/60000]\n",
            "loss: 0.678924  [38400/60000]\n",
            "loss: 0.692834  [44800/60000]\n",
            "loss: 0.669326  [51200/60000]\n",
            "loss: 0.642574  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.639570 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.570058  [    0/60000]\n",
            "loss: 0.692096  [ 6400/60000]\n",
            "loss: 0.472829  [12800/60000]\n",
            "loss: 0.717320  [19200/60000]\n",
            "loss: 0.604315  [25600/60000]\n",
            "loss: 0.600857  [32000/60000]\n",
            "loss: 0.664003  [38400/60000]\n",
            "loss: 0.685381  [44800/60000]\n",
            "loss: 0.660738  [51200/60000]\n",
            "loss: 0.630115  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.627999 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.554948  [    0/60000]\n",
            "loss: 0.676603  [ 6400/60000]\n",
            "loss: 0.461112  [12800/60000]\n",
            "loss: 0.706836  [19200/60000]\n",
            "loss: 0.596139  [25600/60000]\n",
            "loss: 0.592984  [32000/60000]\n",
            "loss: 0.650141  [38400/60000]\n",
            "loss: 0.678906  [44800/60000]\n",
            "loss: 0.653363  [51200/60000]\n",
            "loss: 0.618572  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.617382 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.541108  [    0/60000]\n",
            "loss: 0.662311  [ 6400/60000]\n",
            "loss: 0.450489  [12800/60000]\n",
            "loss: 0.697113  [19200/60000]\n",
            "loss: 0.588606  [25600/60000]\n",
            "loss: 0.585976  [32000/60000]\n",
            "loss: 0.637237  [38400/60000]\n",
            "loss: 0.673385  [44800/60000]\n",
            "loss: 0.647103  [51200/60000]\n",
            "loss: 0.607723  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.607615 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.528413  [    0/60000]\n",
            "loss: 0.649111  [ 6400/60000]\n",
            "loss: 0.440791  [12800/60000]\n",
            "loss: 0.688055  [19200/60000]\n",
            "loss: 0.581577  [25600/60000]\n",
            "loss: 0.579649  [32000/60000]\n",
            "loss: 0.625218  [38400/60000]\n",
            "loss: 0.668714  [44800/60000]\n",
            "loss: 0.641864  [51200/60000]\n",
            "loss: 0.597411  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.598619 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.516729  [    0/60000]\n",
            "loss: 0.636938  [ 6400/60000]\n",
            "loss: 0.431854  [12800/60000]\n",
            "loss: 0.679546  [19200/60000]\n",
            "loss: 0.574962  [25600/60000]\n",
            "loss: 0.573878  [32000/60000]\n",
            "loss: 0.614075  [38400/60000]\n",
            "loss: 0.664916  [44800/60000]\n",
            "loss: 0.637434  [51200/60000]\n",
            "loss: 0.587523  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.590329 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.505895  [    0/60000]\n",
            "loss: 0.625723  [ 6400/60000]\n",
            "loss: 0.423634  [12800/60000]\n",
            "loss: 0.671552  [19200/60000]\n",
            "loss: 0.568595  [25600/60000]\n",
            "loss: 0.568521  [32000/60000]\n",
            "loss: 0.603690  [38400/60000]\n",
            "loss: 0.661916  [44800/60000]\n",
            "loss: 0.633725  [51200/60000]\n",
            "loss: 0.578089  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.582670 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.495762  [    0/60000]\n",
            "loss: 0.615396  [ 6400/60000]\n",
            "loss: 0.416002  [12800/60000]\n",
            "loss: 0.663935  [19200/60000]\n",
            "loss: 0.562455  [25600/60000]\n",
            "loss: 0.563451  [32000/60000]\n",
            "loss: 0.594000  [38400/60000]\n",
            "loss: 0.659570  [44800/60000]\n",
            "loss: 0.630601  [51200/60000]\n",
            "loss: 0.569057  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.8%, Avg loss: 0.575578 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.486159  [    0/60000]\n",
            "loss: 0.605825  [ 6400/60000]\n",
            "loss: 0.408884  [12800/60000]\n",
            "loss: 0.656609  [19200/60000]\n",
            "loss: 0.556501  [25600/60000]\n",
            "loss: 0.558512  [32000/60000]\n",
            "loss: 0.584921  [38400/60000]\n",
            "loss: 0.657775  [44800/60000]\n",
            "loss: 0.627905  [51200/60000]\n",
            "loss: 0.560315  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.568991 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.477066  [    0/60000]\n",
            "loss: 0.596955  [ 6400/60000]\n",
            "loss: 0.402204  [12800/60000]\n",
            "loss: 0.649486  [19200/60000]\n",
            "loss: 0.550712  [25600/60000]\n",
            "loss: 0.553714  [32000/60000]\n",
            "loss: 0.576451  [38400/60000]\n",
            "loss: 0.656489  [44800/60000]\n",
            "loss: 0.625607  [51200/60000]\n",
            "loss: 0.551808  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.562860 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.468447  [    0/60000]\n",
            "loss: 0.588714  [ 6400/60000]\n",
            "loss: 0.396008  [12800/60000]\n",
            "loss: 0.642745  [19200/60000]\n",
            "loss: 0.545003  [25600/60000]\n",
            "loss: 0.548987  [32000/60000]\n",
            "loss: 0.568496  [38400/60000]\n",
            "loss: 0.655710  [44800/60000]\n",
            "loss: 0.623541  [51200/60000]\n",
            "loss: 0.543538  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.557144 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.460278  [    0/60000]\n",
            "loss: 0.581100  [ 6400/60000]\n",
            "loss: 0.390258  [12800/60000]\n",
            "loss: 0.636349  [19200/60000]\n",
            "loss: 0.539429  [25600/60000]\n",
            "loss: 0.544417  [32000/60000]\n",
            "loss: 0.561030  [38400/60000]\n",
            "loss: 0.655363  [44800/60000]\n",
            "loss: 0.621711  [51200/60000]\n",
            "loss: 0.535532  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.551799 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.452540  [    0/60000]\n",
            "loss: 0.574061  [ 6400/60000]\n",
            "loss: 0.384876  [12800/60000]\n",
            "loss: 0.630254  [19200/60000]\n",
            "loss: 0.533967  [25600/60000]\n",
            "loss: 0.539778  [32000/60000]\n",
            "loss: 0.554073  [38400/60000]\n",
            "loss: 0.655314  [44800/60000]\n",
            "loss: 0.620091  [51200/60000]\n",
            "loss: 0.527813  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.546780 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.445264  [    0/60000]\n",
            "loss: 0.567561  [ 6400/60000]\n",
            "loss: 0.379827  [12800/60000]\n",
            "loss: 0.624296  [19200/60000]\n",
            "loss: 0.528549  [25600/60000]\n",
            "loss: 0.535183  [32000/60000]\n",
            "loss: 0.547519  [38400/60000]\n",
            "loss: 0.655430  [44800/60000]\n",
            "loss: 0.618567  [51200/60000]\n",
            "loss: 0.520420  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.542088 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.438315  [    0/60000]\n",
            "loss: 0.561542  [ 6400/60000]\n",
            "loss: 0.375043  [12800/60000]\n",
            "loss: 0.618691  [19200/60000]\n",
            "loss: 0.523196  [25600/60000]\n",
            "loss: 0.530721  [32000/60000]\n",
            "loss: 0.541350  [38400/60000]\n",
            "loss: 0.655732  [44800/60000]\n",
            "loss: 0.617169  [51200/60000]\n",
            "loss: 0.513245  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.537705 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.431701  [    0/60000]\n",
            "loss: 0.555911  [ 6400/60000]\n",
            "loss: 0.370542  [12800/60000]\n",
            "loss: 0.613333  [19200/60000]\n",
            "loss: 0.517957  [25600/60000]\n",
            "loss: 0.526239  [32000/60000]\n",
            "loss: 0.535599  [38400/60000]\n",
            "loss: 0.656185  [44800/60000]\n",
            "loss: 0.615783  [51200/60000]\n",
            "loss: 0.506373  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.533598 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.425370  [    0/60000]\n",
            "loss: 0.550688  [ 6400/60000]\n",
            "loss: 0.366304  [12800/60000]\n",
            "loss: 0.608119  [19200/60000]\n",
            "loss: 0.512853  [25600/60000]\n",
            "loss: 0.521769  [32000/60000]\n",
            "loss: 0.530238  [38400/60000]\n",
            "loss: 0.656694  [44800/60000]\n",
            "loss: 0.614443  [51200/60000]\n",
            "loss: 0.499774  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.529741 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.419316  [    0/60000]\n",
            "loss: 0.545858  [ 6400/60000]\n",
            "loss: 0.362315  [12800/60000]\n",
            "loss: 0.603087  [19200/60000]\n",
            "loss: 0.507880  [25600/60000]\n",
            "loss: 0.517401  [32000/60000]\n",
            "loss: 0.525150  [38400/60000]\n",
            "loss: 0.657154  [44800/60000]\n",
            "loss: 0.613216  [51200/60000]\n",
            "loss: 0.493519  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.526107 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.413504  [    0/60000]\n",
            "loss: 0.541303  [ 6400/60000]\n",
            "loss: 0.358583  [12800/60000]\n",
            "loss: 0.598227  [19200/60000]\n",
            "loss: 0.503010  [25600/60000]\n",
            "loss: 0.513119  [32000/60000]\n",
            "loss: 0.520382  [38400/60000]\n",
            "loss: 0.657572  [44800/60000]\n",
            "loss: 0.612029  [51200/60000]\n",
            "loss: 0.487554  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.522686 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.407860  [    0/60000]\n",
            "loss: 0.537062  [ 6400/60000]\n",
            "loss: 0.355061  [12800/60000]\n",
            "loss: 0.593578  [19200/60000]\n",
            "loss: 0.498207  [25600/60000]\n",
            "loss: 0.508877  [32000/60000]\n",
            "loss: 0.515920  [38400/60000]\n",
            "loss: 0.657921  [44800/60000]\n",
            "loss: 0.610804  [51200/60000]\n",
            "loss: 0.481817  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.519458 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.402477  [    0/60000]\n",
            "loss: 0.533110  [ 6400/60000]\n",
            "loss: 0.351709  [12800/60000]\n",
            "loss: 0.589014  [19200/60000]\n",
            "loss: 0.493564  [25600/60000]\n",
            "loss: 0.504747  [32000/60000]\n",
            "loss: 0.511737  [38400/60000]\n",
            "loss: 0.658209  [44800/60000]\n",
            "loss: 0.609588  [51200/60000]\n",
            "loss: 0.476351  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.516402 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.397293  [    0/60000]\n",
            "loss: 0.529419  [ 6400/60000]\n",
            "loss: 0.348533  [12800/60000]\n",
            "loss: 0.584581  [19200/60000]\n",
            "loss: 0.489037  [25600/60000]\n",
            "loss: 0.500719  [32000/60000]\n",
            "loss: 0.507778  [38400/60000]\n",
            "loss: 0.658410  [44800/60000]\n",
            "loss: 0.608335  [51200/60000]\n",
            "loss: 0.471153  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.513499 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.392277  [    0/60000]\n",
            "loss: 0.525910  [ 6400/60000]\n",
            "loss: 0.345458  [12800/60000]\n",
            "loss: 0.580335  [19200/60000]\n",
            "loss: 0.484673  [25600/60000]\n",
            "loss: 0.496786  [32000/60000]\n",
            "loss: 0.503988  [38400/60000]\n",
            "loss: 0.658484  [44800/60000]\n",
            "loss: 0.607000  [51200/60000]\n",
            "loss: 0.466198  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.510740 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.387388  [    0/60000]\n",
            "loss: 0.522649  [ 6400/60000]\n",
            "loss: 0.342553  [12800/60000]\n",
            "loss: 0.576226  [19200/60000]\n",
            "loss: 0.480473  [25600/60000]\n",
            "loss: 0.492965  [32000/60000]\n",
            "loss: 0.500382  [38400/60000]\n",
            "loss: 0.658390  [44800/60000]\n",
            "loss: 0.605592  [51200/60000]\n",
            "loss: 0.461455  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.508120 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.382648  [    0/60000]\n",
            "loss: 0.519570  [ 6400/60000]\n",
            "loss: 0.339841  [12800/60000]\n",
            "loss: 0.572254  [19200/60000]\n",
            "loss: 0.476342  [25600/60000]\n",
            "loss: 0.489253  [32000/60000]\n",
            "loss: 0.496966  [38400/60000]\n",
            "loss: 0.658164  [44800/60000]\n",
            "loss: 0.604054  [51200/60000]\n",
            "loss: 0.456915  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.505620 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.378038  [    0/60000]\n",
            "loss: 0.516676  [ 6400/60000]\n",
            "loss: 0.337304  [12800/60000]\n",
            "loss: 0.568443  [19200/60000]\n",
            "loss: 0.472359  [25600/60000]\n",
            "loss: 0.485620  [32000/60000]\n",
            "loss: 0.493740  [38400/60000]\n",
            "loss: 0.657735  [44800/60000]\n",
            "loss: 0.602531  [51200/60000]\n",
            "loss: 0.452617  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.503231 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.373573  [    0/60000]\n",
            "loss: 0.513914  [ 6400/60000]\n",
            "loss: 0.334877  [12800/60000]\n",
            "loss: 0.564773  [19200/60000]\n",
            "loss: 0.468528  [25600/60000]\n",
            "loss: 0.482039  [32000/60000]\n",
            "loss: 0.490667  [38400/60000]\n",
            "loss: 0.657088  [44800/60000]\n",
            "loss: 0.600938  [51200/60000]\n",
            "loss: 0.448529  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.500940 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.369243  [    0/60000]\n",
            "loss: 0.511371  [ 6400/60000]\n",
            "loss: 0.332549  [12800/60000]\n",
            "loss: 0.561370  [19200/60000]\n",
            "loss: 0.464712  [25600/60000]\n",
            "loss: 0.478565  [32000/60000]\n",
            "loss: 0.487734  [38400/60000]\n",
            "loss: 0.656354  [44800/60000]\n",
            "loss: 0.599320  [51200/60000]\n",
            "loss: 0.444657  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.498746 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.365061  [    0/60000]\n",
            "loss: 0.508990  [ 6400/60000]\n",
            "loss: 0.330293  [12800/60000]\n",
            "loss: 0.558111  [19200/60000]\n",
            "loss: 0.460996  [25600/60000]\n",
            "loss: 0.475263  [32000/60000]\n",
            "loss: 0.484898  [38400/60000]\n",
            "loss: 0.655546  [44800/60000]\n",
            "loss: 0.597706  [51200/60000]\n",
            "loss: 0.440941  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.496638 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.361073  [    0/60000]\n",
            "loss: 0.506733  [ 6400/60000]\n",
            "loss: 0.328167  [12800/60000]\n",
            "loss: 0.554979  [19200/60000]\n",
            "loss: 0.457422  [25600/60000]\n",
            "loss: 0.472088  [32000/60000]\n",
            "loss: 0.482135  [38400/60000]\n",
            "loss: 0.654697  [44800/60000]\n",
            "loss: 0.596053  [51200/60000]\n",
            "loss: 0.437550  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.494617 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.357266  [    0/60000]\n",
            "loss: 0.504583  [ 6400/60000]\n",
            "loss: 0.326124  [12800/60000]\n",
            "loss: 0.551904  [19200/60000]\n",
            "loss: 0.453957  [25600/60000]\n",
            "loss: 0.469062  [32000/60000]\n",
            "loss: 0.479444  [38400/60000]\n",
            "loss: 0.653815  [44800/60000]\n",
            "loss: 0.594462  [51200/60000]\n",
            "loss: 0.434357  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.492672 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.353544  [    0/60000]\n",
            "loss: 0.502524  [ 6400/60000]\n",
            "loss: 0.324134  [12800/60000]\n",
            "loss: 0.548997  [19200/60000]\n",
            "loss: 0.450615  [25600/60000]\n",
            "loss: 0.466203  [32000/60000]\n",
            "loss: 0.476884  [38400/60000]\n",
            "loss: 0.652816  [44800/60000]\n",
            "loss: 0.592871  [51200/60000]\n",
            "loss: 0.431351  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.490787 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.350000  [    0/60000]\n",
            "loss: 0.500569  [ 6400/60000]\n",
            "loss: 0.322245  [12800/60000]\n",
            "loss: 0.546185  [19200/60000]\n",
            "loss: 0.447360  [25600/60000]\n",
            "loss: 0.463465  [32000/60000]\n",
            "loss: 0.474390  [38400/60000]\n",
            "loss: 0.651752  [44800/60000]\n",
            "loss: 0.591300  [51200/60000]\n",
            "loss: 0.428520  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.488963 \n",
            "\n",
            "334.55355525016785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###!mkdir drive/MyDrive/CP_data/hw01\n",
        "torch.save(model.state_dict(),'drive/MyDrive/CP_data/hw01/test.pkl')"
      ],
      "metadata": {
        "id": "5zh70J-jJk6O"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = NeuralNetwork()\n",
        "model2.load_state_dict(torch.load('drive/MyDrive/CP_data/hw01/test.pkl'))\n",
        "\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model2.eval()\n",
        "\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model2(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'預測值：\"{predicted}\" / 實際值：\"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfy6KUHoNnd3",
        "outputId": "258c6b65-1370-4901-f2c7-ec6d796a4dad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "預測值：\"Ankle boot\" / 實際值：\"Ankle boot\"\n"
          ]
        }
      ]
    }
  ]
}